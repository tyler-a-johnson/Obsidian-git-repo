





import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.metrics import r2_score

import seaborn as sns
import matplotlib.pylab as plt
%matplotlib inline

# supress some warnings that mess up my slides
import warnings; warnings.simplefilter('ignore')








birth = pd.read_csv("birthweight_reduced.csv")
print(birth.shape)
birth.head() 

#Gestation (weeks)
#mnocig: Cigarettes (number smoked/day) for mother 
#fnocig: Cigarettes (number smoked/day), for father
#mppwt: Pre-pregnancy Weight (pounds)
#fedyrs: Father's education (years)
#bwt: Birth Weight (pounds)
#mage: Maternal Age (years)


p = sns.pairplot(birth[['length', 'Birthweight', 'smoker', 'motherage', 'mheight']], hue='smoker')





p = sns.lmplot(x='mheight', y='length', data=birth, height=5, fit_reg=False)
 #Plot data and regression model fits across a FacetGrid.





birth[['mheight', 'length']].corr()











# create X and y
#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
feature_cols = ['mheight']
X = birth[feature_cols]
y = birth['length'] 

# import, instantiate, fit
from sklearn.linear_model import LinearRegression 
model = LinearRegression() #Ordinary least squares Linear Regression. 
reg=model.fit(X, y) #Fit linear model.
reg.score(X, y) #Return the coefficient of determination of the prediction.
                #The best possible score is 1.0 and it can be negative


print("y = ", model.coef_, "* x + ", model.intercept_) 








xmin = X.min()
ymin = model.coef_[0] * xmin + model.intercept_ #[0] means the value for the first feature
xmax = X.max()
ymax = model.coef_[0] * xmax + model.intercept_


plt.figure(figsize=(7,7))
plt.plot([xmin, xmax], [ymin, ymax])  # single line
plt.scatter(X, y)  # original data 


# Same thing with lmplot (linear model plot)
# shade area, https://stackoverflow.com/questions/61522386/sns-regplot-shows-a-shaded-area-for-the-regression-which-does-not-make-sense
p = sns.lmplot(x='mheight', y='length', data=birth, height=5)





print("Predicted value for x =", xmin[0], "is", model.predict([xmin]))
yhat = model.predict(X) #Predict using the linear model.
yhat





mse = ((y - model.predict(X))**2).mean() #y = birth['length'] 
print(mse, np.sqrt(mse)) #mean squared error, 0 is the best








#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html
#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html
from sklearn.metrics import mean_squared_error, r2_score

predicted = model.predict(X)
print("MSE:", mean_squared_error(y, predicted))
print("R^2:", r2_score(y, predicted))





# create X and y
feature_cols = ['mheight', 'motherage']
X = birth[feature_cols]
y = birth['length']
 
model = LinearRegression()
reg2=model.fit(X, y)
reg2.score(X, y) #The best possible score is 1.0 and it can be negative


print("y = ", model.coef_, "* X + ", model.intercept_)


predicted = model.predict(X)
print("MSE:", mean_squared_error(y, predicted)) #MSE: the best value is 0.0
print("R^2:", r2_score(y, predicted)) #R2: Best possible score is 1.0 and it can be negative
                                











feature_cols = ['mheight', 'motherage']
X = birth[feature_cols]
y = birth['length']

X_train = X[:30] #from start to 30 rows
print(X_train)
y_train = y[:30]
X_test = X[30:] #from 30 to the end of rows
print(X_test)
y_test = y[30:]





model = LinearRegression()
model.fit(X_train, y_train)
predicted = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, predicted))
print("R^2:", r2_score(y_test, predicted)) # if the model does not fit the trend (due to selecting biased samples), https://stats.stackexchange.com/questions/12900/when-is-r-squared-negative





from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #20% for test
print(X_train.shape, X_test.shape)


model = LinearRegression()
model.fit(X_train, y_train)
predicted = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, predicted))
print("R^2:", r2_score(y_test, predicted))








birth[['mheight', 'smoker', 'length', 'Birthweight', 'LowBirthWeight', 'lowbwt']].head()


feature_cols = ['mheight', 'smoker']
X = birth[feature_cols]
y = birth['Birthweight']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()
model.fit(X_train, y_train)
predicted = model.predict(X_test)
print("y = ", model.coef_, "* X + ", model.intercept_)
print("MSE:", mean_squared_error(y_test, predicted))
print("R^2:", r2_score(y_test, predicted))





iris = sns.load_dataset('iris') #https://github.com/mwaskom/seaborn-data
sns.pairplot(iris, hue='species')


#Python astype() method enables us to set or convert the data type of an existing data column in a dataset or a data frame.
# another way: https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html
iris['is_setosa'] = (iris['species'] == 'setosa').astype('int64')
iris['is_versicolor'] = (iris['species'] == 'versicolor').astype('int64')
iris['is_virginica'] = (iris['species'] == 'virginica').astype('int64')
print(iris['is_setosa'])
iris.sort_values('sepal_length')
sns.lmplot(x='sepal_length', y='is_setosa', data=iris, fit_reg=False)


# Fit a linear model
feature_cols = ['sepal_length']
X = iris[feature_cols]
y = iris['is_setosa'] 
model = LinearRegression()
model.fit(X, y)
print("y = ", model.coef_, "* X + ", model.intercept_)






predicted = model.predict(X)
plt.scatter(X, predicted)
plt.xlabel('sepal_length')
plt.ylabel('predicted is_setosa')








#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
from sklearn.linear_model import LogisticRegression

feature_cols = ['sepal_length']
X = iris[feature_cols]
y = iris['is_setosa']
logreg = LogisticRegression(C=1e9) #Inverse of regularization strength
                                 #https://stackoverflow.com/questions/22851316/what-is-the-inverse-of-regularization-strength-in-logistic-regression-how-shoul
reg3=logreg.fit(X, y)
reg3.score(X,y) #Return the mean accuracy on the given test data and labels





# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
probs = logreg.predict_proba(X) [:, 1] #Probability estimates, the returned estimates for all classes are ordered by the label of classes.
#Returns array-like of shape (n_samples, n_classes)
plt.scatter(X, y); plt.scatter(X, probs)
plt.xlabel('sepal_length');plt.ylabel('is_setosa')
#None








# create a table of probability versus odds
table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})
table['odds'] = table.probability/(1 - table.probability)
table


# add log-odds to the table
table['logodds'] = np.log(table.odds)
table














# Set up an experiment with the birth data
feature_cols = np.array(['smoker', 'motherage', 'mheight', 'Gestation', 'fheight', 'mppwt', 'fage', 'fedyrs', 'fnocig'])
X = birth[feature_cols]
y = birth['Birthweight']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)


from sklearn.feature_selection import RFE

estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select=6)  # select 6 features for us
selector = selector.fit(X, y)

supp = selector.get_support() #https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel.get_support
#Get a mask, or integer index, of the features selected.
print("Selected features:", feature_cols[supp])
print("Coeffs:", selector.estimator_.coef_)
# test the model
predicted = selector.predict(X)
print("MSE:", mean_squared_error(y, predicted))
print("R^2:", r2_score(y, predicted))



