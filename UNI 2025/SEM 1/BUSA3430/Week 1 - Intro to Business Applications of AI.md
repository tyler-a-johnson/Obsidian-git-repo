[Week 1 Lecture](Attachments/BUSA6430-3430%20-%20S1%202025%20-%20LECTURE%20week%201.pdf)

# Questions about AI in Business
How is AI defined/understood and used in practice?

Who is using AI and for what purpose? What kind of AI?

How is AI integrated with other organisational practices, such as business process
management, innovation, performance management, knowledge management, digital
transformation, competitive differentiation and new business models?

How to determine the business value of AI?

How to use AI to support ethical & responsible decision making and value creation?

How do we operationalise (implement in practice) high-level responsible AI principles and
guidelines? How do we choose among so many of them? How do we keep up with so
many? Should we even keep up?

What are the unintended consequences of AI for individuals, organisations and/or society
at large?

How is your future work going to be impacted by AI? Will my profession become
obsolete/replaced by AI?

How do you future-proof your AI (and other) knowledge and skills for the unknown and
unknowable future of AI?

Is education/professional development even needed when I can use Generative AI to get
answers to any questions?

What is the future of work, society and humankind in AI-driven word? What is your role in
it, as a business/AI or any professional?

## Pros and Cons
**Pros**
• Increased efficiency
• Increased productivity
• Improved decision-making
• Better use of employees’ skills and talents
• Empowerment of people in workplaces and society
• Creation of new products and services
• Improved safety of humans (in different contexts)
• Improved health and well-being
• New employment opportunities
• & ….more benefits emerging

**Cons**
• Job Displacement
• Skill loss
• Privacy & other ethical concerns
• Bias in AI Systems
• Discrimination and denial of service/opportunities
• Security Risks
• Harm caused by ‘AI Hallucination”
• Treat to human safety (in various contexts)
• Lack of Explainability
• Human rights issues
• Unregulated responsibilities
• Amplification and propagation of harm across AI systems
• Misinformed beliefs about AI – “AI is neutral”, “AI is right”; “AI is THE answer”
• & …new types of harm are emerging

## Some Ethical Concerns
• Accountability: Who is responsible for harm caused by AI?
• Transparency: AI as a "black box" problem
• Fairness: AI can perpetuate and exacerbate existing and create new ones
• Privacy: AI systems often rely on large amounts of personal data
• Autonomy: Impact of AI on human autonomy and agency (ability to act)

# The Turing Test
![[The Turing Test]] 


# History of AI

- ***1950 - Birth of AI***
	- Summer conference at Dartmouth, sponsored by DARPA
- ***1954 - 1974***
	- Well funded research to fulfil the promise of developing "AI systems equivalent of human brain"
	- **Notable developments** - ELIZA (Early chatbot)
```ad-note
title: ELIZA Example Conversation

**Human**: Well, my boyfriend made me come here.

**ELIZA**: Your boyfriend made you come here?

**Human**: He says I'm depressed much of the time.

**ELIZA**: I am sorry to hear you are depressed.

**Human**: It's true. I'm unhappy.

**ELIZA**: Do you think coming here will help you not
to be unhappy?

***Extremely primitive - Uses input and more-or-less repeats it back to the user***
```

- ***1974 - 1980 - AI Winter***
	- Lack of enthusiasm over unmet expectations
	- **Notable Developments:** Expert Systems, Focus on developing commercial AI products

- ***1980 - 1987 - Renewed Enthusiasm***
	- **Notable Developments:** The first Neural Network (kinda) with 4 nodes

- ***Early 1990 - 2000 - Second AI Winter***
	- Investors (scum) put off by the term 'voice recognition' which failed to live up to expectations previously.
	- Term avoided by most Comp Sci and Software Engineers
	- **Notable Developments:** "Machine Learning" applications for analyzing data
	- ***11th May 1997 - Deep Blue beats Kasparov***
	- ***2005 - Stanford robot drives autonomously 131 miles across a desert road***

- ***2005 - 2020 - Behind the Scenes AI development*** 
	- Rise of deep learning and **big data**
	- AI-based applications: data mining, search engines, recommendation feeds
	- **Notable Development:** 2017 Transformer Architecture proposed by Google (enabled LLMs and Nvidia GPU tech)

- ***2020 - Today***
	- Rapid development of LLMs
	- OpenAI releases ChatGPT (Generative Pre-Trained Transformer - 3) in 2020 (transformer model of DNN)

## Informal: Isaac Asimov's "Three Laws of Robotics"
Mostly function-less and easily broken. Just here for flavor really.

1. A robot may not injure a human being or, through inaction, allow a human
being to come to harm.
2. A robot must obey orders given it by human beings except where such
orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not
conflict with the First or Second Law.

## First national-level AI 'soft law' - UK Gov 2011
1. Robots are multi-use tools. Robots should not be designed solely or primarily to kill
or harm humans, except in the interests of national security.
2. Humans, not Robots, are responsible agents. Robots should be designed and
operated as far as practicable to comply with existing laws, fundamental rights and
freedoms, including privacy.
3. Robots are products. They should be designed using processes which assure their
safety and security.
1. Robots are manufactured artifacts. They should not be designed in a deceptive
way to exploit vulnerable users; instead their machine nature should be transparent.
2. The person with legal responsibility for a robot should be attributed.

# Definitions of AI

**Technical**
- “AI is defined as the simulation of human intelligence processes by machines,
especially computer systems”
- “AI is a field that combines computer science and robust datasets to enable
problem-solving”
- “AI refers to applying advanced analysis and logic-based techniques, including
machine learning (ML), to interpret events, support and automate decisions, and
take actions”

**Socio-Technical**
- AI is seen as a socio-technical system, i.e., the combination of the technical component (i.e., the code and—if used—the data) and socio elements (i.e., the stakeholders responsible for the system and the society in which the system is deployed)

# Framework for AI Risk Assessment
```ad-quote

“ Governments in 78 countries across six continents have worked with research scientists and others to develop draft legislation aimed at making at AI safe, though the work is still evolving”  [MIT Press](https://mitsloan.mit.edu/ideas-made-to-matter/a-framework-assessing-ai-risk)
```

```ad-success
title: Low Risk (Green Light)
AI applications without potential for harm; have been used for many years
(e.g product recommendations, customer-chatbots etc.)
```
```ad-warning
title: High Risk (Yellow Light)
AI applications that impact on different aspects of human lives and have a
potential to create any harm (note: ‘Any’ harm; There is no such a thing as
“little bit of harm”) – many business applications are in this category
Require human oversight; implement monitoring; continuous testing
```
```ad-danger
title: Prohibited/Very High Risk
AI applications that should be prohibited (e.g. social scoring, public
surveillance, autonomous weapons)
```

To do list for Week 2  
• Find examples of low, high-risk and very-high risk AI applications and bring  
them to the class