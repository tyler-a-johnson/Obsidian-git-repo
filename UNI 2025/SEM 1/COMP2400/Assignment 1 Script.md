---
date: 2025-03-24
tags: 
status: Incomplete
Relevant Questions: 
Relevant Notes: 
Relevant Links:
---
Introduction: Relational Thinking in Data Science

AI systems are mistakenly perceived as impartial tools that are free of bias and only improve productivity and judgement. But as studies such as "Algorithmic Injustice: A Relational Ethics Approach" by Birhane demonstrate, these systems have the potential to reinforce social, cultural, and historical inequalities.
In this presentation, I will contrast the dominant rational-individualistic view with the relational view, and outline each perspectives strengths and draw-backs. I will explain why centering the disproportionately impacted is essential and emphasise the importance of understanding over mere prediction. I will also outline actionable mitigation strategies, as described by Birhane, to promote a more ethical approach to AI development. By shifting our approach to data science, we can create AI systems that are more transparent, and better suited to minimise harm to marginalised communities.

---

Slide 1 – The Rational-Individualistic View

The rational-individualistic perspective, as describe by Birhane, prioritises "reason and logical coherence as superior for knowledge production". It assumes that an objective truth exists independently of social and historical contexts, and statically with respect to time. In AI development, this leads to an emphasis on technical solutions such as 'de-biasing' datasets. The rational-individualistic view aims to analyse information from a distant and theoretically 'objective' perspective.

As Birhane argues, this detachment ignores the complexity of human experiences and associated systemic inequalities.  For example, research from ProPublica in 2016 shows criminal risk assessment algorithms used in some states in America and elsewhere consistently predict minorities as being more likely to re-offend compared to white people. This leads to significant real-world consequences, such as unjust rehabilitation measures, and continued marginalisation of already vulnerable communities. Birhane argues this is a result of the rational-individualistic perspective's failure to account for historical biases and the influences that inform data.

---

Slide 2 – The Relational View

Now we consider the relational view. Rooted in traditions such as Afro-feminist epistemology, enactive cognitive science, and the Sub-Saharan African Ubuntu philosophy, this perspective sees knowledge as inherently connected and contextual. Instead of treating data as an abstract and static entity, the relational view acknowledges that data is shaped by the people and communities who produce, interpret, and are affected by it.

Afro-feminist epistemology emphasizes that knowledge production is an active and engaged practice. It asserts that lived experiences provide the most reliable form of knowledge, especially with respect to marginalised groups experiencing discrimination and social injustice. Concrete experiences take precedence over abstract reasoning, ensuring that ethical AI development is guided by those most affected by algorithmic harm. Similarly, enactive cognitive science argues that knowledge emerges through dynamic interaction between the knower and the known. This opposes the rationalist notion of an objective frame of reference, and instead frames understanding as relational and deeply embedded in social, historical, and societal contexts. Relational thinking aims to describe the complexities of human experience and information rather than reducing them to static data points.

---

Slide 3 – Centering the Disproportionately Impacted

Of Birhane's four key tenets, two stand out as exemplifying the contrast between the rational and relational worldview. These tenets also address methods through which data scientists and AI developers can narrow the divide between the traditional western rational view, and the holistic relational view.

Centering the impacted means recognizing that marginalized communities—who face the greatest risks from algorithmic systems—possess an ‘epistemic privilege’ in identifying harm. Their voices should guide the development process to ensure that AI does not reinforce existing inequalities but instead serves as a tool for social good. I agree with Birhane’s assessment as real-world AI failures—such as facial recognition misidentifying dark-skinned individuals—demonstrate how systems built without input from affected groups reinforce harm. Centering the impacted allows for more ethical and inclusive AI development.

---

Slide 4 - Prioritizing Understanding

Second, prioritizing understanding over prediction means shifting away from a narrow focus on improving accuracy or reducing statistical bias. Instead, we must ask deeper questions about why certain patterns appear in data and how they reflect broader societal structures. For example, instead of merely reducing bias in hiring algorithms, we should examine why disparities in employment exist in the first place and address those root causes. I agree with Birhane’s argument as prioritizing prediction without understanding the root causes of bias leads to superficial fixes. This tenet requires both the rational and relational approach be utilised to identify and address systemic biases.

Together, these tenets promote a holistic and ethical approach to AI development that moves beyond surface-level fixes.

---

Slide 5 – Mitigation Strategies & Implementation Challenges

Algorithmic injustices can be mitigated through relational approaches that involve affected communities, contextual analysis, and continuous evaluation. Community-driven design ensures marginalised groups participate in AI development, while qualitative research, such as ethnographic studies, helps uncover root causes of bias. Iterative impact assessments refine AI models based on lived experiences, preventing static fixes.

These strategies require interdisciplinary collaboration among social scientists, policymakers, AI developers, and marginalised communities. Teams should establish participatory research initiatives where affected communities guide ethical AI standards. Companies and institutions must treat impact assessments as ongoing responsibilities, not one-time audits.

This implementation faces obstacles. Companies may resist due to cost and efficiency concerns, as they rush to push products to market. While non-marginalised end users’ lack of awareness reduces public pressure for accountability. Overcoming these barriers requires universities to embed relational ethics in their teachings, companies to commit to responsible AI, and greater public advocacy for ethical AI systems.

---

**Conclusion**

As we continue to develop AI systems, it is crucial to integrate relational ethics into research and practice. Implementing both a rational and relational approach is necessary for identifying and minimising harm.